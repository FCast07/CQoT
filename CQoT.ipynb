{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1484f396-bbd7-44bc-8885-686fece38dcd",
   "metadata": {},
   "source": [
    "# **CQoT Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c646ab-d736-4745-901c-254cb04e66ee",
   "metadata": {},
   "source": [
    "# **CQoT Pipeline as a Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc1d21-250d-441d-9e44-cc98e84c9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, accelerate, flash_attn\n",
    "import os, re\n",
    "import subprocess\n",
    "#remember to upgrade the transformers library to the latest update\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "class CQoT_Pipeline:\n",
    "    #when instantiating the class, you must specify a model, but you can also customise attention implementation and torch_dtype\n",
    "    def __init__(self, model_id, attn_implementation=\"eager\",torch_dtype=\"auto\"):\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", attn_implementation=attn_implementation, torch_dtype=torch_dtype, trust_remote_code=True)\n",
    "        self.llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.llm_model, tokenizer=self.llm_tokenizer)\n",
    "\n",
    "    ###########  CLASS VARIABLES DEFINITIONS ###########\n",
    "    #class variable for reasoning plan generation\n",
    "    generation_config = {\n",
    "        \"return_full_text\": False, #only return the generated text without prepending input\n",
    "        \"temperature\": 0.8, #determines the 'creativity' of the model, lower value = more deterministic \n",
    "        \"top_p\": 0.95, #enables nucleus sampling. Keeps cumulative probability of top choices ≤ 0.95\n",
    "        \"do_sample\": True, #ensures sampling is used instead of greedy decoding\n",
    "        \"max_new_tokens\":2000 #limits the number of new tokens generated in the output\n",
    "    }\n",
    "\n",
    "    #class variable for CQs probing and final output generation\n",
    "    generation_config_zero_temperature = {\n",
    "        \"return_full_text\": False, #only return the generated text without prepending input\n",
    "        \"temperature\": 0.2, #determines the 'creativity' of the model, lower value = more deterministic \n",
    "        \"top_p\": 0.95, #enables nucleus sampling. Keeps cumulative probability of top choices ≤ 0.95\n",
    "        \"do_sample\": True, #ensures sampling is used instead of greedy decoding\n",
    "        \"max_new_tokens\":2000 #limits the number of new tokens generated in the output\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########  CQoT:Plan Reasoning Steps ###########\n",
    "    #function to plan reasoning steps\n",
    "    def get_plan(self, user_prompt):\n",
    "        system_instruction= \"\"\"You are a critical and analytical reasoner. Carefully read the user prompt. \n",
    "        Instead of replying, provide your step by step reasoning, clearly dividing and outlining the different steps. \n",
    "        Each step must be very specific, easy to follow and the set of premises should logically lead to a truthful conclusion. \n",
    "        Do NOT provide the final answer (but don't write down 'do not provide the final answer' in the reasoning process).\"\"\"\n",
    "\n",
    "        #specify prompt\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"user\", \"content\": user_prompt+ \"Remember not to provide the final answer, just spell out the reasoning plan.\"},\n",
    "        ]\n",
    "        \n",
    "        #prompt the model to generate content\n",
    "        response = self.pipe(messages, **self.generation_config)\n",
    "        #return model response\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########  CQoT:Assess Reasoning Steps ###########\n",
    "    #function to assess reasoning steps\n",
    "    def assess_plan(self, reasoning_steps, user_prompt):\n",
    "        system_instruction= \"\"\"You are an analytical and critical reasoner. Assess the provided reasoning steps against the following questions using the user prompt as a context. \n",
    "        Reply simply with 'Yes' or 'No' for each question. Be very strict and strongly critical and answer 'No' when in doubts. Do NOT answer or reply to anything else than this.\n",
    "        1.Does the reasoning process start with clearly defined premises?\n",
    "        2.Are the premises supported by evidence or accepted facts?\n",
    "        3.Does the reasoning process use logical connections between premises and conclusions?\n",
    "        4.Are the logical connections used in the reasoning process valid?\n",
    "        5.Does the reasoning process avoid fallacies or logical errors?\n",
    "        6.Is the conclusion logically derived from the premises?\n",
    "        7.Is the reasoning process consistent with established knowledge or principles?\n",
    "        8.Does the reasoning process lead to a conclusion that is plausible and reasonable?\"\"\"\n",
    "\n",
    "        #specify input\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"assistant\", \"content\": reasoning_steps},\n",
    "        {\"role\": \"user\", \"content\": user_prompt+ \"Remember: you MUST reply to EACH question. The replies should be ONLY 'Yes' or 'No'\"},\n",
    "        ]\n",
    "        \n",
    "        #prompt the model to generate content\n",
    "        response = self.pipe(messages, **self.generation_config_zero_temperature)\n",
    "        #return model response\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###########  CQoT:Actual Output via Reasoning Plan ###########\n",
    "    #function to assess reasoning steps\n",
    "    def actual_output(self, reasoning_steps, user_prompt):\n",
    "        system_instruction= \"\"\"You are a skilled, critical and analytical reasoner. You always reason in steps according to the provided instruction. \n",
    "        Give an answer to the user prompt by utterly and strictly following the protocol established by the reasoning steps. Here are the stages of what you have to do:\n",
    "        1) Carefully read and understand the protocol detailed by the reasoning steps. Then, start your reasoning.\n",
    "        2) Each intermediate conclusion of your reasoning MUST be yielded by the truthfulness of its respective premises.\n",
    "        3) Each established conclusion MUST NOT contradict other information. After each step, look back at previous steps and check that you are not contradicting what you previously deduced.\n",
    "        4) Your final reply MUST be logically entailed by the reasoning steps and each established conclusion. Meticulously ensure that your reply is consistent with your reasoning.\"\"\"\n",
    "\n",
    "        #specify input\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"assistant\", \"content\": reasoning_steps},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        #prompt the model to generate content\n",
    "        response = self.pipe(messages, **self.generation_config_zero_temperature)\n",
    "        #return model response\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "    \n",
    "\n",
    "    #########  MAIN METHOD THAT ACTIVATES CQoT_Pipeline ###########\n",
    "    #set verbose=False to prevent printing intermediate steps\n",
    "    def activate(self, user_prompt, verbose=True):\n",
    "      reasoning_plan = self.get_plan(user_prompt)\n",
    "      if verbose==True:\n",
    "        print(\"#############################################\")\n",
    "        print(f'PLAN: {reasoning_plan}')\n",
    "      verified_plan = self.assess_plan(reasoning_plan, user_prompt)\n",
    "      if verbose==True:\n",
    "        print(\"#############################################\")\n",
    "        print(f'VERIFICATION: {verified_plan}')\n",
    "      assessment = re.findall(\"Yes\", verified_plan)\n",
    "      while_counter = 0\n",
    "      while len(assessment)<7:\n",
    "          if while_counter<4:\n",
    "              reasoning_plan = self.get_plan(user_prompt)\n",
    "              verified_plan = self.assess_plan(reasoning_plan, user_prompt)\n",
    "              assessment = re.findall(\"Yes\", verified_plan)\n",
    "              if verbose==True:\n",
    "                print(f'PLAN: {reasoning_plan}')\n",
    "                print(f'VERIFICATION: {verified_plan}' + f'\\n counter: {str(while_counter)}')\n",
    "              while_counter += 1\n",
    "          else:\n",
    "              reasoning_plan = self.get_plan(user_prompt)\n",
    "              verified_plan = self.assess_plan(reasoning_plan, user_prompt)\n",
    "              assessment = re.findall(\"Yes\", verified_plan)\n",
    "              if verbose==True:\n",
    "                print(f'PLAN: {reasoning_plan}')\n",
    "                print(f'VERIFICATION: {verified_plan}' + f'\\n counter: {str(while_counter)}')\n",
    "              while_counter += 1\n",
    "              if len(assessment)>4:\n",
    "                  break\n",
    "              elif while_counter==10:\n",
    "                  break\n",
    "              \n",
    "      final_plan = reasoning_plan\n",
    "      output = self.actual_output(final_plan, user_prompt) \n",
    "      print(\"#############################################\")\n",
    "      print(f'OUTPUT: {output}')\n",
    "      return output\n",
    "\n",
    "\n",
    "    \n",
    "    #########  METHOD TO GET REASONING_STEP ONLY WITHOUT CQs PROBING ###########\n",
    "    def reasoning_step_only(self, user_prompt):\n",
    "        reasoning_plan = self.get_plan(user_prompt)\n",
    "        output = self.actual_output(reasoning_plan, user_prompt)\n",
    "        print(\"#############################################\")\n",
    "        print(f'OUTPUT: {output}')\n",
    "        return output\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b8a33-fa9e-4383-a67c-87a09ee06c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE: CQoT = CQoT_Pipeline(\"meta-llama/Llama-3.1-70B-Instruct\", attn_implementation='flash_attention_2',torch_dtype=torch.float16)\n",
    "CQoT = CQoT_Pipeline(\"\") #specify at least the model" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e5352-e1ba-4fd9-8506-04280465e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_output = CQoT.activate(\"\"\" \"Each problem consists of three statements. Based on the first two statements, the third statement may be true, false, or uncertain.\\n1. Oranges cost more than apples.\\n2. Oranges cost less than bananas.\\n3. Bananas cost more than apples and bananas cost more than orange.\\nIf the first two statements are true, then the third statement is\" \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
