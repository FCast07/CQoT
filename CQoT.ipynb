{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1484f396-bbd7-44bc-8885-686fece38dcd",
   "metadata": {},
   "source": [
    "# **CQoT Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c646ab-d736-4745-901c-254cb04e66ee",
   "metadata": {},
   "source": [
    "# **CQoT Pipeline as a Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cc1d21-250d-441d-9e44-cc98e84c9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, accelerate, flash_attn\n",
    "import os, re\n",
    "import subprocess\n",
    "#remember to upgrade the transformers library to the latest update\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "class CQoT_Pipeline:\n",
    "    #when instantiating the class, you have to specify a model and can customise attention implementiation and torch_dtype\n",
    "    def __init__(self, model_id, attn_implementation=\"eager\",torch_dtype=\"auto\"):\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", attn_implementation=attn_implementation, torch_dtype=torch_dtype, trust_remote_code=True)\n",
    "        self.llm_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.llm_model, tokenizer=self.llm_tokenizer)\n",
    "\n",
    "    ###########  CLASS VARIABLES DEFINITIONS ###########\n",
    "    #class variable for reasoning plan generation\n",
    "    generation_config = {\n",
    "        \"return_full_text\": False, #only return the generated text without prepending input.\n",
    "        \"temperature\": 0.8, #determines the 'creativity' of the model, lower value = more deterministic \n",
    "        \"top_p\": 0.95, #enables nucleus sampling. Keeps cumulative probability of top choices ≤ 0.95.\n",
    "        \"do_sample\": True, #ensures sampling is used instead of greedy decoding.\n",
    "        \"max_new_tokens\":2000 #limits the number of new tokens generated in the output.\n",
    "    }\n",
    "\n",
    "    #class variable for CQs probing and final output generation\n",
    "    generation_config_zero_temperature = {\n",
    "        \"return_full_text\": False, #only return the generated text without prepending input.\n",
    "        \"temperature\": 0.2, #determines the 'creativity' of the model, lower value = more deterministic \n",
    "        \"top_p\": 0.95, #enables nucleus sampling. Keeps cumulative probability of top choices ≤ 0.95.\n",
    "        \"do_sample\": True, #ensures sampling is used instead of greedy decoding.\n",
    "        \"max_new_tokens\":2000 #limits the number of new tokens generated in the output.\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########  CQoT:Plan Reasoning Steps ###########\n",
    "    #function to plan reasoning steps\n",
    "    def get_plan(self, user_prompt):\n",
    "        system_instruction= \"\"\"You are a critical and analytical reasoner. Carefully read the user prompt. \n",
    "        Instead of replying, provide your step by step reasoning, clearly dividing and outlining the different steps. \n",
    "        Each step must be very specific, easy to follow and the set of premises should logically lead to a truthful conclusion. \n",
    "        Do NOT provide the final answer (but don't write down 'do not provide the final answer' in the reasoning process).\"\"\"\n",
    "\n",
    "        #specify prompt\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"user\", \"content\": user_prompt+ \"Remember not to provide the final answer, just spell out the reasoning plan.\"},\n",
    "        ]\n",
    "        \n",
    "        #prompt the model to generate content\n",
    "        response = self.pipe(messages, **self.generation_config)\n",
    "        #return model response\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########  CQoT:Assess Reasoning Steps ###########\n",
    "    #function to assess reasoning steps\n",
    "    def assess_plan(self, reasoning_steps, user_prompt):\n",
    "        system_instruction= \"\"\"You are an analytical and critical reasoner. Assess the provided reasoning steps against the following questions using the user prompt as a context. \n",
    "        Reply simply with 'Yes' or 'No' for each question. Be very strict and strongly critical and answer 'No' when in doubts. Do NOT answer or reply to anything else than this.\n",
    "        1.Does the reasoning process start with clearly defined premises?\n",
    "        2.Are the premises supported by evidence or accepted facts?\n",
    "        3.Does the reasoning process use logical connections between premises and conclusions?\n",
    "        4.Are the logical connections used in the reasoning process valid?\n",
    "        5.Does the reasoning process avoid fallacies or logical errors?\n",
    "        6.Is the conclusion logically derived from the premises?\n",
    "        7.Is the reasoning process consistent with established knowledge or principles?\n",
    "        8.Does the reasoning process lead to a conclusion that is plausible and reasonable?\"\"\"\n",
    "\n",
    "        #specify input\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"assistant\", \"content\": reasoning_steps},\n",
    "        {\"role\": \"user\", \"content\": user_prompt+ \"Remember: you MUST reply to EACH question. The replies should be ONLY 'Yes' or 'No'\"},\n",
    "        ]\n",
    "        \n",
    "        #prompt the model to generate content\n",
    "        response = self.pipe(messages, **self.generation_config_zero_temperature)\n",
    "        #return model response\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###########  CQoT:Actual Output via Reasoning Plan ###########\n",
    "    #function to assess reasoning steps\n",
    "    def actual_output(self, reasoning_steps, user_prompt):\n",
    "        system_instruction= \"\"\"You are a skilled, critical and analytical reasoner. You always reason in steps according to the provided instruction. \n",
    "        Give an answer to the user prompt by utterly and strictly following the protocol established by the reasoning steps. Here are the stages of what you have to do:\n",
    "        1) Carefully read and understand the protocol detailed by the reasoning steps. Then, start your reasoning.\n",
    "        2) Each intermediate conclusion of your reasoning MUST be yielded by the truthfulness of its respective premises.\n",
    "        3) Each established conclusion MUST NOT contradict other information. After each step, look back at previous steps and check that you are not contradicting what you previously deduced.\n",
    "        4) Your final reply MUST be logically entailed by the reasoning steps and each established conclusion. Meticulously ensure that your reply is consistent with your reasoning.\"\"\"\n",
    "\n",
    "        #specify input\n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"assistant\", \"content\": reasoning_steps},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        #prompt the model to generate content\n",
    "        response = self.pipe(messages, **self.generation_config_zero_temperature)\n",
    "        #return model response\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "    \n",
    "\n",
    "    #########  MAIN METHOD THAT ACTIVATES CQoT_Pipeline ###########\n",
    "    #set verbose=False to prevent printing intermediate steps\n",
    "    def activate(self, user_prompt, verbose=True):\n",
    "      reasoning_plan = self.get_plan(user_prompt)\n",
    "      if verbose==True:\n",
    "        print(\"#############################################\")\n",
    "        print(f'PLAN: {reasoning_plan}')\n",
    "      verified_plan = self.assess_plan(reasoning_plan, user_prompt)\n",
    "      if verbose==True:\n",
    "        print(\"#############################################\")\n",
    "        print(f'VERIFICATION: {verified_plan}')\n",
    "      assessment = re.findall(\"Yes\", verified_plan)\n",
    "      while_counter = 0\n",
    "      while len(assessment)<7:\n",
    "          if while_counter<4:\n",
    "              reasoning_plan = self.get_plan(user_prompt)\n",
    "              verified_plan = self.assess_plan(reasoning_plan, user_prompt)\n",
    "              assessment = re.findall(\"Yes\", verified_plan)\n",
    "              if verbose==True:\n",
    "                print(f'VERIFICATION: {verified_plan}' + f'\\n counter: {str(while_counter)}')\n",
    "              while_counter += 1\n",
    "          else:\n",
    "              reasoning_plan = self.get_plan(user_prompt)\n",
    "              verified_plan = self.assess_plan(reasoning_plan, user_prompt)\n",
    "              assessment = re.findall(\"Yes\", verified_plan)\n",
    "              if verbose==True:\n",
    "                print(f'VERIFICATION: {verified_plan}' + f'\\n counter: {str(while_counter)}')\n",
    "              while_counter += 1\n",
    "              if len(assessment)>4:\n",
    "                  break\n",
    "              elif while_counter==10:\n",
    "                  break\n",
    "              \n",
    "      final_plan = reasoning_plan\n",
    "      output = self.actual_output(final_plan, user_prompt) \n",
    "      print(\"#############################################\")\n",
    "      print(f'OUTPUT: {output}')\n",
    "      return output\n",
    "\n",
    "\n",
    "    \n",
    "    #########  METHOD TO GET REASONING_STEP ONLY WITHOUT CQs PROBING ###########\n",
    "    def reasoning_step_only(self, user_prompt):\n",
    "        reasoning_plan = self.get_plan(user_prompt)\n",
    "        output = self.actual_output(reasoning_plan, user_prompt)\n",
    "        print(\"#############################################\")\n",
    "        print(f'OUTPUT: {output}')\n",
    "        return output\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c8bfaf-7d05-4b59-8f14-dd72d84c92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (4.41.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /tmpdir/job/1627737.undefined/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.1\n",
      "    Uninstalling huggingface-hub-0.23.1:\n",
      "      Successfully uninstalled huggingface-hub-0.23.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.1\n",
      "    Uninstalling transformers-4.41.1:\n",
      "      Successfully uninstalled transformers-4.41.1\n",
      "Successfully installed huggingface-hub-0.26.3 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443b8a33-fa9e-4383-a67c-87a09ee06c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352cfdfc54db4061bbcd309989e3c2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes.\n",
    "#pipeline = CQoT_Pipeline(\"meta-llama/Llama-3.1-70B-Instruct\", attn_implementation='flash_attention_2')\n",
    "pipeline = CQoT_Pipeline(\"/lustre/scratch/mmm1397/models_Llama3_1_70B/\", attn_implementation='flash_attention_2',torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "955e5352-e1ba-4fd9-8506-04280465e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "PLAN: **Step 1: Analyze the First Statement**\n",
      "- Identify the relationship between the costs of oranges and apples.\n",
      "- Determine that the statement implies oranges are more expensive than apples.\n",
      "- Represent this relationship symbolically: Orange > Apple\n",
      "\n",
      "**Step 2: Analyze the Second Statement**\n",
      "- Identify the relationship between the costs of oranges and bananas.\n",
      "- Determine that the statement implies oranges are less expensive than bananas.\n",
      "- Represent this relationship symbolically: Orange < Banana\n",
      "\n",
      "**Step 3: Combine the Information from Steps 1 and 2**\n",
      "- Create a combined symbolic representation of the relationships: Banana > Orange > Apple\n",
      "- Analyze the implications of this combined relationship on the costs of the three fruits.\n",
      "\n",
      "**Step 4: Evaluate the Third Statement**\n",
      "- Break down the third statement into its components: \n",
      "  1. Bananas cost more than apples.\n",
      "  2. Bananas cost more than oranges.\n",
      "- Compare these components with the combined relationship from Step 3.\n",
      "- Assess whether the information from Steps 1 and 2 supports, contradicts, or leaves uncertain the components of the third statement.\n",
      "\n",
      "**Step 5: Determine the Truth Value of the Third Statement**\n",
      "- Based on the evaluation in Step 4, determine whether the third statement is true, false, or uncertain given the information provided in the first two statements.\n",
      "- Consider whether any part of the third statement contradicts or cannot be confirmed by the first two statements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "VERIFICATION: 1. No\n",
      "2. No\n",
      "3. No\n",
      "4. No\n",
      "5. No\n",
      "6. No\n",
      "7. No\n",
      "8. No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: 1. Does the reasoning process start with clearly defined premises? Yes\n",
      "2. Are the premises supported by evidence or accepted facts? No\n",
      "3. Does the reasoning process use logical connections between premises and conclusions? Yes\n",
      "4. Are the logical connections used in the reasoning process valid? Yes\n",
      "5. Does the reasoning process avoid fallacies or logical errors? Yes\n",
      "6. Is the conclusion logically derived from the premises? Yes\n",
      "7. Is the reasoning process consistent with established knowledge or principles? No\n",
      "8. Does the reasoning process lead to a conclusion that is plausible and reasonable? Yes\n",
      " counter: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: 1. Does the reasoning process start with clearly defined premises? Yes\n",
      "2. Are the premises supported by evidence or accepted facts? No\n",
      "3. Does the reasoning process use logical connections between premises and conclusions? Yes\n",
      "4. Are the logical connections used in the reasoning process valid? Yes\n",
      "5. Does the reasoning process avoid fallacies or logical errors? Yes\n",
      "6. Is the conclusion logically derived from the premises? Yes\n",
      "7. Is the reasoning process consistent with established knowledge or principles? Yes\n",
      "8. Does the reasoning process lead to a conclusion that is plausible and reasonable? Yes\n",
      " counter: 1\n",
      "#############################################\n",
      "OUTPUT: **Step 1: Analyze the given statements**\n",
      "- Statement 1: Oranges cost more than apples (O > A).\n",
      "- Statement 2: Oranges cost less than bananas (O < B).\n",
      "- Statement 3: Bananas cost more than apples and bananas cost more than oranges (B > A and B > O).\n",
      "\n",
      "**Step 2: Determine the relationship between apples, oranges, and bananas based on the first two statements**\n",
      "- From Statement 1, we know that oranges are more expensive than apples (O > A).\n",
      "- From Statement 2, we know that bananas are more expensive than oranges (B > O).\n",
      "\n",
      "**Step 3: Use transitive property to compare bananas and apples**\n",
      "- Given that O > A (from Statement 1) and B > O (from Statement 2), we can infer that B > O > A.\n",
      "- This implies that bananas are more expensive than apples (B > A).\n",
      "\n",
      "**Step 4: Evaluate the truth of the first part of Statement 3**\n",
      "- The first part of Statement 3 claims that bananas cost more than apples (B > A), which is supported by the analysis in Step 3.\n",
      "\n",
      "**Step 5: Evaluate the truth of the second part of Statement 3**\n",
      "- The second part of Statement 3 claims that bananas cost more than oranges (B > O), which is directly supported by Statement 2.\n",
      "\n",
      "**Step 6: Draw a conclusion about the validity of Statement 3 based on the analysis**\n",
      "- Since both parts of Statement 3 are supported by the analysis, Statement 3 is true.\n",
      "\n",
      "The final answer is: **True**\n"
     ]
    }
   ],
   "source": [
    "pipeline_output = pipeline.activate(\"\"\" \"Each problem consists of three statements. Based on the first two statements, the third statement may be true, false, or uncertain.\\n1. Oranges cost more than apples.\\n2. Oranges cost less than bananas.\\n3. Bananas cost more than apples and bananas cost more than orange.\\nIf the first two statements are true, then the third statement is\" \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752642e4-fd7e-49c6-a78b-54ac80a8a2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
